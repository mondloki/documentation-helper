{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.env',\n",
       " '.git',\n",
       " '.gitignore',\n",
       " 'download_langchain_docs.py',\n",
       " 'ingestion.py',\n",
       " 'langchain_docs',\n",
       " 'README.md',\n",
       " 'requirements.txt',\n",
       " 'test.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('langchain_docs/api.python.langchain.com/en/latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agents',\n",
       " 'ai21_api_reference.html',\n",
       " 'airbyte_api_reference.html',\n",
       " 'anthropic_api_reference.html',\n",
       " 'astradb_api_reference.html',\n",
       " 'aws_api_reference.html',\n",
       " 'azure_dynamic_sessions_api_reference.html',\n",
       " 'callbacks',\n",
       " 'chains',\n",
       " 'chat_models',\n",
       " 'chroma_api_reference.html',\n",
       " 'cohere_api_reference.html',\n",
       " 'community_api_reference.html',\n",
       " 'core_api_reference.html',\n",
       " 'couchbase_api_reference.html',\n",
       " 'elasticsearch_api_reference.html',\n",
       " 'embeddings',\n",
       " 'evaluation',\n",
       " 'exa_api_reference.html',\n",
       " 'experimental_api_reference.html',\n",
       " 'fireworks_api_reference.html',\n",
       " 'google_genai_api_reference.html',\n",
       " 'google_vertexai_api_reference.html',\n",
       " 'groq_api_reference.html',\n",
       " 'hub',\n",
       " 'huggingface_api_reference.html',\n",
       " 'ibm_api_reference.html',\n",
       " 'indexes',\n",
       " 'langchain_api_reference.html',\n",
       " 'memory',\n",
       " 'milvus_api_reference.html',\n",
       " 'mistralai_api_reference.html',\n",
       " 'model_laboratory',\n",
       " 'mongodb_api_reference.html',\n",
       " 'nomic_api_reference.html',\n",
       " 'nvidia_ai_endpoints_api_reference.html',\n",
       " 'openai_api_reference.html',\n",
       " 'output_parsers',\n",
       " 'pinecone_api_reference.html',\n",
       " 'postgres_api_reference.html',\n",
       " 'prompty_api_reference.html',\n",
       " 'qdrant_api_reference.html',\n",
       " 'retrievers',\n",
       " 'robocorp_api_reference.html',\n",
       " 'runnables',\n",
       " 'smith',\n",
       " 'storage',\n",
       " 'text_splitters_api_reference.html',\n",
       " 'together_api_reference.html',\n",
       " 'voyageai_api_reference.html',\n",
       " 'weaviate_api_reference.html']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {\"chat_history\" : []}\n",
    "\n",
    "val[\"chat_history\"].append((\"human\", \"Whats up ?\"))\n",
    "val[\"chat_history\"].append((\"ai\", \"Not much?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': [('human', 'Whats up ?'), ('ai', 'Not much?')]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pprint(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "human\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Question: What did i just ask you ? \n",
      "\n",
      "Context: LangChain...\n",
      "LangChain Python API Reference#\n",
      "Welcome to the LangChain Python API reference. This is a reference for all\n",
      "langchain-x packages.\n",
      "For user guides see https://python.langchain.com.\n",
      "For the legacy API reference hosted on ReadTheDocs see https://api.python.langchain.com/.\n",
      "Base packages#\n",
      "Core\n",
      "langchain-core: 0.2.39\n",
      "core/index.html\n",
      "Langchain\n",
      "langchain: 0.2.16\n",
      "langchain/index.html\n",
      "Text Splitters\n",
      "langchain-text-splitters: 0.2.4\n",
      "text_splitters/index.html\n",
      "Community\n",
      "langchain-community: 0.2.16\n",
      "community/index.html\n",
      "Experimental\n",
      "langchain-experimental: 0.0.65\n",
      "experimental/index.html\n",
      "\n",
      "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
      "production-grade programs that harness the power of LLMs.\n",
      "Programs created using LCEL and LangChain Runnables inherently support\n",
      "synchronous, asynchronous, batch, and streaming operations.\n",
      "Support for async allows servers hosting LCEL based programs to scale better\n",
      "for higher concurrent loads.\n",
      "Batch operations allow for processing multiple inputs in parallel.\n",
      "Streaming of intermediate outputs, as theyâre being generated, allows for\n",
      "creating more responsive UX.\n",
      "\n",
      "Defaults to False.\n",
      "param tags: Optional[List[str]] = NoneÂ¶\n",
      "Optional list of tags associated with the chain. Defaults to None.\n",
      "These tags will be associated with each call to this chain,\n",
      "and passed as arguments to the handlers defined in callbacks.\n",
      "You can use these to eg identify a specific instance of a chain with its use case.\n",
      "param verbose: bool [Optional]Â¶\n",
      "Whether or not run in verbose mode. In verbose mode, some intermediate logs\n",
      "will be printed to the console. Defaults to the global verbose value,\n",
      "accessible via langchain.globals.get_verbose().\n",
      "\n",
      "Chain that interprets a prompt and executes python code to do math.\n",
      "Note: this class is deprecated. See below for a replacement implementationusing LangGraph. The benefits of this implementation are:\n",
      "Uses LLM tool calling features;\n",
      "Support for both token-by-token and step-by-step streaming;\n",
      "Support for checkpointing and memory of chat history;\n",
      "Easier to modify or extend (e.g., with additional tools, structured responses, etc.)\n",
      "Install LangGraph with:\n",
      "pip install -U langgraph\n",
      "import math\n",
      "from typing import Annotated, Sequence\n",
      "from langchain_core.messages import BaseMessage \n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(\"Human: \\nhuman\\n\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\nQuestion: What did i just ask you ? \\n\\nContext: LangChain...\\nLangChain Python API Reference#\\nWelcome to the LangChain Python API reference. This is a reference for all\\nlangchain-x packages.\\nFor user guides see https://python.langchain.com.\\nFor the legacy API reference hosted on ReadTheDocs see https://api.python.langchain.com/.\\nBase packages#\\nCore\\nlangchain-core: 0.2.39\\ncore/index.html\\nLangchain\\nlangchain: 0.2.16\\nlangchain/index.html\\nText Splitters\\nlangchain-text-splitters: 0.2.4\\ntext_splitters/index.html\\nCommunity\\nlangchain-community: 0.2.16\\ncommunity/index.html\\nExperimental\\nlangchain-experimental: 0.0.65\\nexperimental/index.html\\n\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\nSupport for async allows servers hosting LCEL based programs to scale better\\nfor higher concurrent loads.\\nBatch operations allow for processing multiple inputs in parallel.\\nStreaming of intermediate outputs, as theyâre being generated, allows for\\ncreating more responsive UX.\\n\\nDefaults to False.\\nparam tags: Optional[List[str]] = NoneÂ¶\\nOptional list of tags associated with the chain. Defaults to None.\\nThese tags will be associated with each call to this chain,\\nand passed as arguments to the handlers defined in callbacks.\\nYou can use these to eg identify a specific instance of a chain with its use case.\\nparam verbose: bool [Optional]Â¶\\nWhether or not run in verbose mode. In verbose mode, some intermediate logs\\nwill be printed to the console. Defaults to the global verbose value,\\naccessible via langchain.globals.get_verbose().\\n\\nChain that interprets a prompt and executes python code to do math.\\nNote: this class is deprecated. See below for a replacement implementationusing LangGraph. The benefits of this implementation are:\\nUses LLM tool calling features;\\nSupport for both token-by-token and step-by-step streaming;\\nSupport for checkpointing and memory of chat history;\\nEasier to modify or extend (e.g., with additional tools, structured responses, etc.)\\nInstall LangGraph with:\\npip install -U langgraph\\nimport math\\nfrom typing import Annotated, Sequence\\nfrom langchain_core.messages import BaseMessage \\n\\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [('human', 'What is langchain ?'), ('ai', 'LangChain is a framework for developing applications powered by large language models (LLMs).  It provides tools for building chains of LLMs and other components.  The provided text focuses on its Python API and associated packages.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_str = \"\"\n",
    "for history in chat_history:\n",
    "    chat_history_str += f\"{history[0]} : {history[1]}\"\n",
    "    chat_history_str += \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human : What is langchain ?\n",
      "ai : LangChain is a framework for developing applications powered by large language models (LLMs).  It provides tools for building chains of LLMs and other components.  The provided text focuses on its Python API and associated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat_history_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
